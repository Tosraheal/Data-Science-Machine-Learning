{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "extensions": {
          "jupyter_dashboards": {
            "version": 1,
            "views": {
              "grid_default": {
                "col": 0,
                "height": 4,
                "hidden": false,
                "row": 0,
                "width": 4
              },
              "report_default": {
                "hidden": false
              }
            }
          }
        },
        "id": "ffxgtZjJtexQ"
      },
      "source": [
        "# Project: Wrangling and Analyze Data\n",
        "*This project aims to perform Data Wrangling and Exploratory Data Analysis on the archived data of WeRateDogs® Twitter account.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZLL45Tdtexb"
      },
      "source": [
        "## Data Gathering\n",
        "In the cells below, **all** three pieces of data for this project are gathered and loaded in the notebook. **Note:** the methods required to gather each data are different.\n",
        "1. Direct import of the WeRateDogs Twitter archive data (twitter_archive_enhanced.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "extensions": {
          "jupyter_dashboards": {
            "version": 1,
            "views": {
              "grid_default": {
                "hidden": true
              },
              "report_default": {
                "hidden": true
              }
            }
          }
        },
        "id": "bkTMKwKftexc"
      },
      "outputs": [],
      "source": [
        "#Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import json\n",
        "from IPython.display import Image\n",
        "import datetime\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "% matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "PHyUmmfptexe",
        "outputId": "4a4f1c38-383c-4652-8a2c-c6a78c84bf5e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4001e0d999a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdt1_archive\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'twitter-archive-enhanced-2.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#downloading from uploaded document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdt1_archive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'twitter-archive-enhanced-2.csv'"
          ]
        }
      ],
      "source": [
        "dt1_archive  = pd.read_csv('twitter-archive-enhanced-2.csv') #downloading from uploaded document\n",
        "dt1_archive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQXtWWqetexf"
      },
      "source": [
        "2. Used the Requests library to download the tweet image prediction (image_predictions.tsv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NTGn96btexf"
      },
      "outputs": [],
      "source": [
        "#Using the request library to download the flat file image_predictions\n",
        "url = \"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\"\n",
        "\n",
        "r = requests.get(url)\n",
        "with open('image-predictions.tsv', mode ='wb') as file:\n",
        "    file.write(r.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCzuCaCFtexg"
      },
      "outputs": [],
      "source": [
        "dt2_image = pd.read_csv('image-predictions.tsv', sep = '\\t')\n",
        "dt2_image.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SwJgM_atexh"
      },
      "source": [
        "3. Used the provided (tweet_json.txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_KlSzxKtexh"
      },
      "outputs": [],
      "source": [
        "#Reading the tweet-Json file\n",
        "\n",
        "dt = pd.read_csv('tweet-json.txt', delimiter = \"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlnB2YHGtexi"
      },
      "outputs": [],
      "source": [
        "#we write this list into a txt file:\n",
        "dt_list = []\n",
        "with open('tweet-json.txt') as file:\n",
        "    for line in file:\n",
        "        dt_list.append(json.loads(line))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "_LZQ_5xItexi"
      },
      "outputs": [],
      "source": [
        "dt_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfvqwG2ntexj"
      },
      "outputs": [],
      "source": [
        "\n",
        "tweet_json = pd.DataFrame(dt_list, columns = ['id', 'favorite_count','retweet_count'\n",
        "                                                           ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "NvGQ73n5texk"
      },
      "outputs": [],
      "source": [
        "tweet_json.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBxtTwd7texk"
      },
      "outputs": [],
      "source": [
        "#changing column 'id' to tweet_id using pandas rename\n",
        "tweet_json = tweet_json.rename(columns = {'id':'tweet_id'})\n",
        "tweet_json.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eko5TKH4texm"
      },
      "outputs": [],
      "source": [
        "tweet_json.to_csv('tweet_json.csv', index=False) #storing the file in csv\n",
        "\n",
        "data_tweet = pd.read_csv('tweet_json.csv')\n",
        "data_tweet.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "extensions": {
          "jupyter_dashboards": {
            "version": 1,
            "views": {
              "grid_default": {
                "col": 4,
                "height": 4,
                "hidden": false,
                "row": 28,
                "width": 4
              },
              "report_default": {
                "hidden": false
              }
            }
          }
        },
        "id": "E-YQOFmltexn"
      },
      "source": [
        "## Assessing Data\n",
        "In this section, I was able to detect and document at least **eight (8) quality issues and two (2) tidiness issue**. Displaying **both** visual assessment and programmatic assessement to assess the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6mkt61stexn"
      },
      "source": [
        "**(Visual assessment) Each of the three data gathered is displayed for visual assessment purpose.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_4gmEzAtexn"
      },
      "outputs": [],
      "source": [
        "dt1_archive #Visual assessment twitter archive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6rk9zlhtexo"
      },
      "source": [
        "A quality issue identified visually in twitter-archive-enhanced-2.csv (dt1_archive): Invalid names or non-standard names in the name column, this will require further analysis using the .value_counts() method to examine the frequency of the error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRf33R82texo"
      },
      "outputs": [],
      "source": [
        "dt2_image #Visual assessment image predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a7IBuzrtexo"
      },
      "outputs": [],
      "source": [
        "# This is an image for tweet_id 666049248165822465 Visual assessment\n",
        "Image(url = 'https://pbs.twimg.com/media/CT5IQmsXIAAKY4A.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGiSr1P2texp"
      },
      "source": [
        "The tweet_id is unique key to the tweet and not really to the rated dog. its better to filter for unique pictures of the dogs, which will also remove duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsBpGIsatexp"
      },
      "outputs": [],
      "source": [
        "data_tweet #Visual assessment for tweet_json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp9ochvNtexp"
      },
      "source": [
        "**(Programmatic assessment) Using Pandas' functions to assess each gathered data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37PXlUAXtexq"
      },
      "source": [
        "Assessing the twitter Archive Enhanced File (dt1_archive) to generate data quality and tidiness issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-t3QCdetexq"
      },
      "outputs": [],
      "source": [
        "dt1_archive.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZulS7wh5texq"
      },
      "source": [
        "**Some quality issues:**\n",
        "The retweeted_status_id, retweeted_status_user_id, in_reply_to_status_id, and in_reply_to_user_id might give wrong results if not cleaned because it will duplicate. The same dog picture in each retweet or reply. It's good to remove the tweet_id's.\n",
        "\n",
        "The dog \"Sierra\" appears twice due to the retweet.\n",
        "\n",
        "Wron datatypes e.g object instead of date time for timestamp etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1bVaekotexq"
      },
      "outputs": [],
      "source": [
        "# Subsetting the dt1_archive to find a retweet.\n",
        "dt1_archive[dt1_archive.name == 'Sierra']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp_7UQTbtexr"
      },
      "source": [
        "*The Tweet_ids with duplicate retweet status will be dropped.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAmR-Jc3texr"
      },
      "outputs": [],
      "source": [
        "dt1_archive.name.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eR2EQM9Atexr"
      },
      "source": [
        "**Quality issue:** Some names are invalid and will be dropped (None,a, the, an)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh9FMiEPtexr"
      },
      "outputs": [],
      "source": [
        "dt1_archive.rating_numerator.describe() #checking the characteistics of the rating numerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLrnmXZUtexr"
      },
      "outputs": [],
      "source": [
        "dt1_archive[dt1_archive.rating_numerator<10].count()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv4VTHzwtexs"
      },
      "outputs": [],
      "source": [
        "dt1_archive[dt1_archive.rating_numerator<10].tweet_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcQa4o0rtexs"
      },
      "source": [
        "**Quality issue:** tweet_ids with numerators lower than 10 are incorrect due to the peculiarity of the WeRateDogs twitter page, they will be dropped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOHxJfBbtexs"
      },
      "outputs": [],
      "source": [
        "dt1_archive.rating_denominator.describe() #checking the characteistics of the rating denominator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCnADKwGtexs"
      },
      "outputs": [],
      "source": [
        "dt1_archive[dt1_archive.rating_denominator<10].count()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5Ki4slFtext"
      },
      "outputs": [],
      "source": [
        "dt1_archive[dt1_archive.rating_denominator<10].tweet_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oTWOy32text"
      },
      "outputs": [],
      "source": [
        "dt1_archive[dt1_archive.tweet_id == 810984652412424192]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZANO_T3text"
      },
      "source": [
        "**Quality issue:** tweet_ids with denominators lower than 10 are incorrect as the standard is 10 and the three tweet ids in this category must be input errors and will be dropped.\n",
        "example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8mPTOvftext"
      },
      "outputs": [],
      "source": [
        "dt1_archive[dt1_archive.tweet_id == 835246439529840640] #An example with zero denominator instead of ten"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-CjhnJUtexu"
      },
      "source": [
        "**Assessing the image-predictions(dt2_image) to generate data quality and tidiness issues**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aK9S20ZRtexu"
      },
      "outputs": [],
      "source": [
        "# Overall of the dt2_image.\n",
        "dt2_image.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7mM6Q2btexu"
      },
      "outputs": [],
      "source": [
        "# Checking for Duplicated imagesjpg_url.\n",
        "sum(dt2_image.jpg_url.duplicated())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brbwjF9htexu"
      },
      "source": [
        "**Some quality issues:** The tweet_id to be\tConverted to string.\n",
        "The jpg_url\thas duplicated images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRJiDu_ytexu"
      },
      "source": [
        "**Assessing the tweet_json file(data_tweet) to generate data quality and tidiness issues**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7pESWZEtexu"
      },
      "outputs": [],
      "source": [
        "data_tweet.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIrHyBMdtexu"
      },
      "source": [
        "No issue is noticed from Tweet_json file.\n",
        "\n",
        "*However a general tidiness issue is that the files are related and yet broken into 3. The primary key which is the tweet_id will be used to join them during cleaning.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX_dnA9Rtexv"
      },
      "source": [
        "### Quality issues\n",
        "**Assessing the twitter Archive Enhanced File**\n",
        "1. There are 181 retweets in the the file.\n",
        "\n",
        "2. Presence of invalid dog names (None, a, The, an, etc.)\n",
        "\n",
        "3. Numerators with ratings less than 10 about 440\n",
        "\n",
        "4. Denominators with zero rating\n",
        "\n",
        "5. Columns in wrong data type object to datetime\n",
        "\n",
        "6. Tweet id data type is integer instead of spring\n",
        "\n",
        "**Assessing the Image Prediction File**\n",
        "\n",
        "7. The jpg_urls are duplicated\n",
        "\n",
        "8. Some of the tweet_ids has no images total (2075 rows instead 2356)\n",
        "\n",
        "9. Some of the dogs names 'p's start with small letters and others capital letters\n",
        "\n",
        "**Assessing the Tweet_json File**\n",
        "\n",
        "10. Missing entries (Only 2354 entries, instead of 2356)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "extensions": {
          "jupyter_dashboards": {
            "version": 1,
            "views": {
              "grid_default": {
                "col": 0,
                "height": 7,
                "hidden": false,
                "row": 40,
                "width": 12
              },
              "report_default": {
                "hidden": false
              }
            }
          }
        },
        "id": "TOdhD9pltexv"
      },
      "source": [
        "### Tidiness issues\n",
        "1. The dog data is separated into four different columns\n",
        "\n",
        "2. The data files are related but are in different dataframes divide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "extensions": {
          "jupyter_dashboards": {
            "version": 1,
            "views": {
              "grid_default": {
                "col": 4,
                "height": 4,
                "hidden": false,
                "row": 32,
                "width": 4
              },
              "report_default": {
                "hidden": false
              }
            }
          }
        },
        "id": "-y-uAMlOtexv"
      },
      "source": [
        "## Cleaning Data\n",
        "In this section, **All** of the issues documented while assessing was addressed. \n",
        "\n",
        "A copy of the original data was done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og4vHDfItexv"
      },
      "outputs": [],
      "source": [
        "# Making copies of original pieces of data\n",
        "\n",
        "# Copying the dt1_archive.\n",
        "clean_archive = dt1_archive.copy()\n",
        "\n",
        "# Copying the dt2_image.\n",
        "clean_image = dt2_image.copy()\n",
        "\n",
        "# Copying the data_tweet. \n",
        "clean_tweet = data_tweet.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AjLQgdBtexv"
      },
      "outputs": [],
      "source": [
        "clean_archive.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdYQvf59texv"
      },
      "outputs": [],
      "source": [
        "clean_image.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-JSCvTStexw"
      },
      "outputs": [],
      "source": [
        "clean_tweet.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGBIOQXTtexw"
      },
      "source": [
        "### Issue #1: Cleaning Tidiness Issues "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQIl7HiUtexw"
      },
      "source": [
        "#### Define:  The dog's data are in four separate columns \n",
        "**This will be merged into one 'dog_states'**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57Rc-9xwtexx"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVxrbWfCtexx"
      },
      "outputs": [],
      "source": [
        "# Extract the text from the columns into the new dog_states column\n",
        "clean_archive['dog_states'] = clean_archive['text'].str.extract('(doggo|floofer|pupper|puppo)')\n",
        "clean_archive.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rz4ihs9xtexx"
      },
      "outputs": [],
      "source": [
        "#dropping unneccessary columns (doggo|floofer|pupper|puppo)\n",
        "clean_archive = clean_archive.drop(columns = ['doggo', 'floofer', 'pupper', 'puppo'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo-xZvILtexx"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LiMdNJatexx"
      },
      "outputs": [],
      "source": [
        "clean_archive.dog_states.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "extensions": {
          "jupyter_dashboards": {
            "version": 1,
            "views": {
              "grid_default": {
                "hidden": true
              },
              "report_default": {
                "hidden": true
              }
            }
          }
        },
        "id": "vAS5TPJxtexy"
      },
      "source": [
        "#### Define: The data files are related but are in different dataframes divide\n",
        "**Merging all the files into one, based on tweet_id as primary key.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DueTKiOtexy"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oxvE7Zxtexy"
      },
      "outputs": [],
      "source": [
        "#Using the pandas merge function to join the files, into one dataframe.\n",
        "df = pd.merge(clean_archive, clean_tweet, on='tweet_id', how='left') #df = clean_archive + clean_tweet\n",
        "df = pd.merge(df, clean_image, on='tweet_id', how='left') #df = df + clean_image\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCVdMfg_texy"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZyBpgF0texy"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpGFnIC5texy"
      },
      "source": [
        "### Issue #2: Cleaning Some Quality Issues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "extensions": {
          "jupyter_dashboards": {
            "version": 1,
            "views": {
              "grid_default": {
                "hidden": true
              },
              "report_default": {
                "hidden": true
              }
            }
          }
        },
        "id": "SL9PRCeLtexz"
      },
      "source": [
        "#### Define: Q1 There are 181 retweets in the the file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWDutSwItexz"
      },
      "source": [
        "**Only rows where retweeted_status_id is null will be kept.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8GmCv_utexz"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ws1oeGC5texz"
      },
      "outputs": [],
      "source": [
        "# Select rows with only where retweeted_status_id is Null.\n",
        "df = df[df.retweeted_status_id.isnull()]\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inskSRyXtexz"
      },
      "outputs": [],
      "source": [
        "#dropping the unneccessary related columns to retweet\n",
        "df = df.drop(columns=['retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp',\n",
        "                      'in_reply_to_status_id', 'in_reply_to_user_id' ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtVH6XbStexz"
      },
      "source": [
        "#### Test: Expect all rows and columns deviod of retweet duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SuKWpL0tex0"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "extensions": {
          "jupyter_dashboards": {
            "version": 1,
            "views": {
              "grid_default": {
                "hidden": true
              },
              "report_default": {
                "hidden": true
              }
            }
          }
        },
        "id": "f3MmafmXtex0"
      },
      "source": [
        "#### Define: Q2 Presence of invalid dog names (None, a, The, an, etc.) \n",
        "**Converting invalid names to None**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC2byrJitex0"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPVLkDeqtex0"
      },
      "outputs": [],
      "source": [
        "# Initialization of variable.\n",
        "invalid_names = []\n",
        "\n",
        "# Looping to find ordinary words.\n",
        "for index in df.name:\n",
        "    # Checking every name starting with lowercase.\n",
        "    if index.islower():\n",
        "        # If yes will append to invalid_names.\n",
        "        invalid_names.append(index)\n",
        "\n",
        "# This list will filter only unique values\n",
        "invalid_names = list(set(invalid_names))\n",
        "\n",
        "# Printing non-standard/ non names.\n",
        "invalid_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnTBWPX5tex0"
      },
      "outputs": [],
      "source": [
        "# Loop to replace each non standard name (invalid_name).\n",
        "for index in invalid_names:\n",
        "    df.name.replace(index,\"None\",inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76o8kE19tex1"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgzFytGctex1"
      },
      "outputs": [],
      "source": [
        "df.name.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1AfcDy7tex1"
      },
      "outputs": [],
      "source": [
        "sum(df.name.isnull())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "extensions": {
          "jupyter_dashboards": {
            "version": 1,
            "views": {
              "grid_default": {
                "hidden": true
              },
              "report_default": {
                "hidden": true
              }
            }
          }
        },
        "id": "Mixo73estex1"
      },
      "source": [
        "#### Define: Q3&4 Dealing with numerators with ratings less than 10 about 440 & denominators with zero rating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGNM7aRrtex1"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgq-6jbitex1"
      },
      "source": [
        "**Standardize the dog ratings:\n",
        "Converting to float, Regularize the the ratings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQGOjvq3tex1"
      },
      "outputs": [],
      "source": [
        "df['rating_numerator'] = df['rating_numerator'].astype(float)\n",
        "df['rating_denominator'] = df['rating_denominator'].astype(float)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18SlE_Uytex2"
      },
      "outputs": [],
      "source": [
        "# For the loop to gather all the text,indices,and ratings for tweets having decimal numerator\n",
        "decimal_rating_text = []\n",
        "decimal_rating_index = []\n",
        "ratings_in_decimals = []\n",
        "\n",
        "\n",
        "for i, text in df['text'].iteritems():\n",
        "    if bool(re.search('\\d+\\.\\d+\\/\\d+', text)):\n",
        "        decimal_rating_text.append(text)\n",
        "        decimal_rating_index.append(i)\n",
        "        ratings_in_decimals.append(re.search('\\d+\\.\\d+', text).group())\n",
        "\n",
        "# The ratings with decimals        \n",
        "decimal_rating_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uRLgCUDtex2"
      },
      "outputs": [],
      "source": [
        "#The indices of the ratings above (having decimal)\n",
        "decimal_rating_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKlqr8gFtex2"
      },
      "outputs": [],
      "source": [
        "#Correctly converting the above decimal ratings to float\n",
        "df.loc[decimal_rating_index[0],'rating_numerator'] = float(ratings_in_decimals[0])\n",
        "df.loc[decimal_rating_index[1],'rating_numerator'] = float(ratings_in_decimals[1])\n",
        "df.loc[decimal_rating_index[2],'rating_numerator'] = float(ratings_in_decimals[2])\n",
        "df.loc[decimal_rating_index[3],'rating_numerator'] = float(ratings_in_decimals[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e__OenEctex2"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xtRREDGtex3"
      },
      "outputs": [],
      "source": [
        "# Testing the indices \n",
        "df.loc[695]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yefWsCkdtex3"
      },
      "outputs": [],
      "source": [
        "Image(url = 'https://pbs.twimg.com/media/CurzvFTXgAA2_AP.jpg') #sample image of index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mcu0qaVFtex3"
      },
      "outputs": [],
      "source": [
        "# A new column called rating is created, calculating the value with new and standardized ratings\n",
        "df['rating'] = df['rating_numerator'] / df['rating_denominator']\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "extensions": {
          "jupyter_dashboards": {
            "version": 1,
            "views": {
              "grid_default": {
                "hidden": true
              },
              "report_default": {
                "hidden": true
              }
            }
          }
        },
        "id": "upFIiQP7tex3"
      },
      "source": [
        "#### Define: Q5 & 6 Columns in wrong data type Timestamp is object instead of datetime, Tweet id data type is integer instead of string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQiaIzYFtex3"
      },
      "source": [
        "**The timestamp variable is an object datatype, this will be coverted to date time format. \n",
        "Pandas datetime was useful in conversion.**\n",
        "\n",
        "**Tweet id data type is in integer and will be converted to string.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI4qNplYtex3"
      },
      "outputs": [],
      "source": [
        "#Converting to datetime format\n",
        "df.timestamp = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S.%f')\n",
        "df.timestamp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6A8LquC3tex4"
      },
      "outputs": [],
      "source": [
        "df.tweet_id = df.tweet_id.astype(str) #from integer to string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpWbLxCFtex4"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qcZligStex4"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "extensions": {
          "jupyter_dashboards": {
            "version": 1,
            "views": {
              "grid_default": {
                "hidden": true
              },
              "report_default": {
                "hidden": true
              }
            }
          }
        },
        "id": "ynOgSReWtex4"
      },
      "source": [
        "#### Define: Q8. Some of the tweet_ids has no images total (2075 rows instead 2356)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhY6A9qUtex4"
      },
      "source": [
        "**Delete rows with missing images**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQSqeAgutex4"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfRzG-J1tex5"
      },
      "outputs": [],
      "source": [
        "df = df[df.jpg_url.notnull()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WqurihBtex5"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVrVm56stex5"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "extensions": {
          "jupyter_dashboards": {
            "version": 1,
            "views": {
              "grid_default": {
                "hidden": true
              },
              "report_default": {
                "hidden": true
              }
            }
          }
        },
        "id": "ypIVsfwatex5"
      },
      "source": [
        "#### Define: Q9. Some of the dogs names 'p's start with small letters and others capital letters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrfkgSRXtex5"
      },
      "source": [
        "**Replace in P, names with '_', from '_' to space**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ebD3W9ytex5"
      },
      "outputs": [],
      "source": [
        "df.p1 = df.p1.str.replace('_', ' ')\n",
        "df.p2 = df.p2.str.replace('_', ' ')\n",
        "df.p3 = df.p3.str.replace('_', ' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbzJnigQtex5"
      },
      "source": [
        "**Convert Lower case to Upper case**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPI67VGKtex6"
      },
      "outputs": [],
      "source": [
        "df.p1 = df.p1.str.title()\n",
        "df.p2 = df.p2.str.title()\n",
        "df.p3 = df.p3.str.title()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj0Uo7--tex6"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWD8sUbLtex6"
      },
      "outputs": [],
      "source": [
        "df.p1.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zYoY5iFtex7"
      },
      "outputs": [],
      "source": [
        "df.p2.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QDDmcNhtex7"
      },
      "outputs": [],
      "source": [
        "df.p3.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlywTwwVtex7"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF12rlsTtex7"
      },
      "source": [
        "**Quality Issues 7 & 10 wont be needed since we have been able to drop major rows and columns with duplicates**\n",
        "*Overall the tidiness and quality of this data has been improved*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_mjY-oZtex7"
      },
      "source": [
        "## Storing Data\n",
        "Save gathered, assessed, and cleaned master dataset to a CSV file named \"twitter_archive_master.csv\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qtaesYwtex8"
      },
      "outputs": [],
      "source": [
        "df.to_csv('twitter_archive_master.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qApVso-tex8"
      },
      "source": [
        "## Analyzing and Visualizing Data\n",
        "In this section, analyzed and visualized wrangled data. Provided at least **three (3) insights and one (1) visualization.**\n",
        "This using the 'df' dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMa0_02Dtex8"
      },
      "outputs": [],
      "source": [
        "monthly_tweets = df.groupby(pd.Grouper(key = 'timestamp', freq = \"M\")).count().reset_index()\n",
        "monthly_tweets = monthly_tweets[['timestamp', 'tweet_id']]\n",
        "monthly_tweets.head()\n",
        "monthly_tweets.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k27AzH8Ztex8"
      },
      "outputs": [],
      "source": [
        "# Plotting time vs. tweets\n",
        "\n",
        "plt.figure(figsize=(10, 10));\n",
        "plt.xlim([datetime.date(2015, 11, 30), datetime.date(2017, 7, 30)]);\n",
        "\n",
        "plt.xlabel('Year and Month')\n",
        "plt.ylabel('Tweets Count')\n",
        "\n",
        "plt.plot(monthly_tweets.timestamp, monthly_tweets.tweet_id);\n",
        "plt.title('We Rate Dogs Tweets over Time');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOFqGC9utex8"
      },
      "source": [
        "WeRateDogs® Twitter account was at its highest tweet count by january 2016. The tweet counts have been maintaining a decline with varying spikes in the mid year of 2016."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "yGRnERmCtex9"
      },
      "outputs": [],
      "source": [
        "# Scatterplot of retweets vs favorite count\n",
        "\n",
        "sns.lmplot(x=\"retweet_count\", \n",
        "           y=\"favorite_count\", \n",
        "           data=df,\n",
        "           size = 5,\n",
        "           aspect=1.3,\n",
        "           scatter_kws={'alpha':1/5});\n",
        "\n",
        "plt.title('Favorite Count vs. Retweet Count');\n",
        "plt.xlabel('WeRateDogs™ Twitter Retweet Count');\n",
        "plt.ylabel('WeRateDogs™ Twitter Favorite Count');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP2oXnHHtex9"
      },
      "source": [
        "There is a linear relationship between favorite count & retweet count of WeRateDogs™ Twitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS2mcSvNtex9"
      },
      "source": [
        "**Percentage of different dog stages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9poAq-3tex-"
      },
      "outputs": [],
      "source": [
        "stage_df = df.dog_states.value_counts()\n",
        "stage_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Nv_3cMqwtex-"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydaHN4vDtex-"
      },
      "outputs": [],
      "source": [
        "#Plotting a pie chart \n",
        "plt.pie(stage_df,\n",
        "       labels = [ 'Pupper','Doggo', 'Puppo', 'Floofer'],\n",
        "       autopct = '%1.1f%%',  #To show percent on plot 1.1 formats the percentage to the tenth place\n",
        "       shadow = True,\n",
        "       explode = (0.1, 0.2, 0.2, 0.3)\n",
        "       )\n",
        "plt.title('Percentage of Dog Stages')\n",
        "plt.axis('equal') #Removing the default tilt from matplotlib pie"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeMoRbx_tex-"
      },
      "source": [
        "Pupper has the highest percentage\n",
        "Floofer has the lowest percentage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fcq9_XDTtex-"
      },
      "source": [
        "### Insights:\n",
        "1. There is a linear relationship between retweet count & favorite count.\n",
        "\n",
        "2. Pupper has the highest percentage\n",
        "\n",
        "3. Floofer has the lowest percentage\n",
        "\n",
        "4. WeRateDogs® Twitter account highest tweet count was at january 2016."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9Il5kIftex_"
      },
      "source": [
        "**N.B: I wasn't able to get an elevated access to twitter developer's but i couldnt get the required api. I used the udacity provided twitter-json.txt.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqkwKW8Etex_"
      },
      "source": [
        "### References\n",
        "\n",
        "https://github.com/AndersonUyekita/ND111_data_science_foundations_02/blob/master/03-Chapter03/00-Project_02/wrangle_act.ipynb\n",
        "https://www.youtube.com/watch?v=0dkzcshJz0k\n",
        "https://docs.python.org/3/library/re.html#:~:text=A%20regular%20expression%20(or%20RE,down%20to%20the%20same%20thing)\n",
        "https://github.com/Abhishek20182/Wrangle-and-Analyze-Data/blob/master/wrangle_act.ipynb\n",
        "https://github.com/zekuva/Udacity-datasets/blob/main/Week_6_project_2_cleaning.ipynb\n",
        "https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html\n",
        "https://stackabuse.com/reading-and-writing-json-to-a-file-in-python/"
      ]
    }
  ],
  "metadata": {
    "extensions": {
      "jupyter_dashboards": {
        "activeView": "report_default",
        "version": 1,
        "views": {
          "grid_default": {
            "cellMargin": 10,
            "defaultCellHeight": 20,
            "maxColumns": 12,
            "name": "grid",
            "type": "grid"
          },
          "report_default": {
            "name": "report",
            "type": "report"
          }
        }
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "wrangle_act.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}